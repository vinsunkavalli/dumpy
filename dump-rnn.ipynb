{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dump-rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant to visualize recurrent neural network activations. The networks used (RNN, LSTM, GRU) are trained on a small portion of the imdb dataset. The networks are then run on a sample sentence and the activations from the predictions are \"dumped\" and saved as both images and an npz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the imdb dataset is loaded using the keras library. We set the maximum sentence length to 100 words and maximum words in the sentence to 10000 words to save both time and increase computation speeds. Then the data is processed so that it may be used by the networks at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "maxlen = 100\n",
    "maxword = 10000\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\", \n",
    "                                                      num_words=maxword, \n",
    "                                                      maxlen=maxlen)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "data = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "labels = np.reshape(y_train, len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the networks used to dump activations are made. There are three models that will be tested in this notebook: regular RNNs, LSTMs, and GRUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "rmodels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = Sequential()\n",
    "\n",
    "rnn.add(layers.Embedding(maxword, 32, input_length=maxlen))\n",
    "rnn.add(layers.SimpleRNN(128, return_sequences=True))\n",
    "rnn.add(layers.SimpleRNN(128))\n",
    "rnn.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "rmodels.append(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = Sequential()\n",
    "\n",
    "lstm.add(layers.Embedding(maxword, 32, input_length=maxlen))\n",
    "lstm.add(layers.LSTM(128, return_sequences=True))\n",
    "lstm.add(layers.LSTM(128))\n",
    "lstm.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "rmodels.append(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru = Sequential()\n",
    "\n",
    "gru.add(layers.Embedding(maxword, 32, input_length=maxlen))\n",
    "gru.add(layers.GRU(128, return_sequences=True))\n",
    "gru.add(layers.GRU(128))\n",
    "gru.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "rmodels.append(gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the models are compiled and trained on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4015 samples, validate on 1721 samples\n",
      "Epoch 1/5\n",
      "4015/4015 [==============================] - 3s - loss: 0.7268 - acc: 0.5083 - val_loss: 0.6914 - val_acc: 0.5415\n",
      "Epoch 2/5\n",
      "4015/4015 [==============================] - 2s - loss: 0.6519 - acc: 0.6234 - val_loss: 0.9292 - val_acc: 0.5189\n",
      "Epoch 3/5\n",
      "4015/4015 [==============================] - 2s - loss: 0.4917 - acc: 0.7666 - val_loss: 0.9420 - val_acc: 0.5317\n",
      "Epoch 4/5\n",
      "4015/4015 [==============================] - 2s - loss: 0.2711 - acc: 0.8909 - val_loss: 1.1238 - val_acc: 0.5334\n",
      "Epoch 5/5\n",
      "4015/4015 [==============================] - 2s - loss: 0.1145 - acc: 0.9606 - val_loss: 1.6581 - val_acc: 0.5218\n",
      "Train on 4015 samples, validate on 1721 samples\n",
      "Epoch 1/5\n",
      "4015/4015 [==============================] - 12s - loss: 0.6179 - acc: 0.6391 - val_loss: 0.4739 - val_acc: 0.7844\n",
      "Epoch 2/5\n",
      "4015/4015 [==============================] - 12s - loss: 0.3250 - acc: 0.8670 - val_loss: 0.6948 - val_acc: 0.7931\n",
      "Epoch 3/5\n",
      "4015/4015 [==============================] - 12s - loss: 0.2195 - acc: 0.9168 - val_loss: 0.3512 - val_acc: 0.8623\n",
      "Epoch 4/5\n",
      "4015/4015 [==============================] - 12s - loss: 0.1450 - acc: 0.9462 - val_loss: 0.3650 - val_acc: 0.8402\n",
      "Epoch 5/5\n",
      "4015/4015 [==============================] - 12s - loss: 0.1049 - acc: 0.9639 - val_loss: 0.3889 - val_acc: 0.8495\n",
      "Train on 4015 samples, validate on 1721 samples\n",
      "Epoch 1/5\n",
      "4015/4015 [==============================] - 10s - loss: 0.6701 - acc: 0.6630 - val_loss: 0.4498 - val_acc: 0.7809\n",
      "Epoch 2/5\n",
      "4015/4015 [==============================] - 10s - loss: 0.3629 - acc: 0.8473 - val_loss: 0.3952 - val_acc: 0.8268\n",
      "Epoch 3/5\n",
      "4015/4015 [==============================] - 10s - loss: 0.2377 - acc: 0.9039 - val_loss: 0.3870 - val_acc: 0.8268\n",
      "Epoch 4/5\n",
      "4015/4015 [==============================] - 10s - loss: 0.1580 - acc: 0.9417 - val_loss: 0.4355 - val_acc: 0.8117\n",
      "Epoch 5/5\n",
      "4015/4015 [==============================] - 10s - loss: 0.1083 - acc: 0.9606 - val_loss: 0.4410 - val_acc: 0.8338\n"
     ]
    }
   ],
   "source": [
    "for model in rmodels:\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    model.fit(data, labels, epochs=5, batch_size=64, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the activations are dumped. First, each model is modified to output each of their layers' outputs. These outputs are added to list which is saved both as graphs and an npz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = np.reshape(data[0], (1, 100))\n",
    "dump = []\n",
    "\n",
    "from keras import models\n",
    "\n",
    "for model in rmodels:\n",
    "    activations = [layer.output for layer in model.layers]  \n",
    "    dump_model = models.Model(inputs=model.input, outputs=activations)  \n",
    "    dump.append(dump_model.predict(input_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.25081924e-02  -2.81243790e-02   1.04718590e-02 ...,  -1.86898783e-02\n",
      "    1.44745102e-02   3.50164510e-02]\n",
      " [ -4.25081924e-02  -2.81243790e-02   1.04718590e-02 ...,  -1.86898783e-02\n",
      "    1.44745102e-02   3.50164510e-02]\n",
      " [ -4.25081924e-02  -2.81243790e-02   1.04718590e-02 ...,  -1.86898783e-02\n",
      "    1.44745102e-02   3.50164510e-02]\n",
      " ..., \n",
      " [ -3.32166627e-02  -2.41237096e-02  -5.97841525e-03 ...,   9.29845264e-05\n",
      "    4.05949913e-03  -1.02841258e-02]\n",
      " [ -2.71964092e-02  -1.82066262e-02   5.65175526e-03 ...,  -3.63797806e-02\n",
      "   -1.72416307e-02  -2.89847571e-02]\n",
      " [ -2.71964092e-02  -1.82066262e-02   5.65175526e-03 ...,  -3.63797806e-02\n",
      "   -1.72416307e-02  -2.89847571e-02]]\n",
      "[[ -1.63121391e-02  -1.61196031e-02  -3.36217694e-04 ...,  -5.83846064e-04\n",
      "   -5.92414774e-02   2.30054744e-02]\n",
      " [ -3.34315188e-02   1.27144274e-03   1.89718008e-02 ...,  -8.62987898e-03\n",
      "   -8.78836140e-02   5.09923398e-02]\n",
      " [ -2.75578294e-02   8.95962957e-03   2.92509552e-02 ...,   6.28189137e-03\n",
      "   -7.34574646e-02   3.51725519e-02]\n",
      " ..., \n",
      " [ -3.62672806e-01  -2.98190176e-01   5.72137356e-01 ...,  -2.00161085e-01\n",
      "   -6.69171140e-02   3.91142935e-01]\n",
      " [ -1.35424703e-01  -2.47646406e-01   5.21922588e-01 ...,  -1.50823966e-01\n",
      "   -1.25074983e-02   3.74720722e-01]\n",
      " [ -3.18299353e-01  -3.33390266e-01   6.30797982e-01 ...,  -1.62792891e-01\n",
      "    4.65218350e-03   3.19449902e-01]]\n",
      "[-0.16389848  0.35857356  0.8430016  -0.8157478  -0.58059597 -0.29440436\n",
      "  0.22251245  0.44718331  0.07135663  0.39996421 -0.08450056  0.50818741\n",
      " -0.03436996  0.65475231 -0.04628413 -0.15072438  0.60691488  0.69252992\n",
      "  0.55160576 -0.61923015 -0.38190538  0.6654985  -0.16184635  0.545964\n",
      "  0.76848042  0.08590037  0.64781344  0.63301098  0.42442015 -0.04279121\n",
      "  0.3272714  -0.02030734  0.63436329  0.42625263 -0.08869185  0.45011353\n",
      " -0.55002141  0.74742442 -0.67525727 -0.73377007 -0.02522574 -0.68506384\n",
      "  0.13523576  0.49932837 -0.52052027  0.42466015 -0.46221733 -0.42532983\n",
      "  0.05490017 -0.54502141  0.42186517 -0.03287287  0.21123362 -0.40826243\n",
      " -0.07887489 -0.03108049  0.2582846   0.31005383 -0.30844927 -0.56484032\n",
      "  0.74706131  0.10645154 -0.16486013  0.03659152  0.80577189 -0.51560521\n",
      " -0.31185687  0.04764716  0.58944166  0.25516668 -0.29644749  0.59450793\n",
      "  0.10143305  0.41092873 -0.45914522 -0.00226398 -0.73154938  0.71824056\n",
      " -0.71919668 -0.13508543  0.20434211  0.35505921  0.08878388  0.6208632\n",
      "  0.49486181 -0.0522914  -0.8164705   0.21380331 -0.63342702  0.31096014\n",
      " -0.63129795 -0.52448326 -0.68362552  0.18366793  0.59693474 -0.14029007\n",
      " -0.29044873  0.75470573 -0.34149638  0.00979255  0.94607884  0.53716129\n",
      "  0.17434774 -0.23808272  0.21868777  0.51482308  0.26920316 -0.13919634\n",
      " -0.63367063  0.42143032  0.44223323 -0.37714362 -0.03812312 -0.06926944\n",
      " -0.49214351 -0.09493702  0.08686309  0.37056369 -0.33117706 -0.57859039\n",
      " -0.41869769 -0.27333543 -0.6385808  -0.76526022 -0.66980374 -0.72788256\n",
      "  0.2862449  -0.09551464]\n",
      "[ 0.0412374]\n",
      "[[-0.00978493 -0.03553245  0.03837135 ..., -0.02972883  0.03397487\n",
      "  -0.03667327]\n",
      " [-0.00978493 -0.03553245  0.03837135 ..., -0.02972883  0.03397487\n",
      "  -0.03667327]\n",
      " [-0.00978493 -0.03553245  0.03837135 ..., -0.02972883  0.03397487\n",
      "  -0.03667327]\n",
      " ..., \n",
      " [ 0.03356881  0.00758529  0.01986274 ...,  0.00851168  0.01459702\n",
      "  -0.03830947]\n",
      " [-0.04706059  0.04164556  0.01231533 ..., -0.0269035  -0.03112894\n",
      "  -0.02103956]\n",
      " [-0.04706059  0.04164556  0.01231533 ..., -0.0269035  -0.03112894\n",
      "  -0.02103956]]\n",
      "[[  2.74446351e-03   5.84038731e-04  -3.50167463e-03 ...,   7.89742393e-04\n",
      "    4.44942096e-04  -7.43136089e-03]\n",
      " [  5.40204300e-03   2.50119669e-03  -6.56726537e-03 ...,   6.19919680e-04\n",
      "    8.40833760e-04  -1.31452736e-02]\n",
      " [  7.81092700e-03   5.02524618e-03  -9.19051934e-03 ...,  -4.62559692e-05\n",
      "    1.16450852e-03  -1.74980499e-02]\n",
      " ..., \n",
      " [  4.16331775e-02   5.63064963e-02  -2.04047374e-02 ...,   3.05173099e-02\n",
      "   -1.70835108e-02   3.19610946e-02]\n",
      " [  4.67174910e-02   5.89741133e-02  -2.22018678e-02 ...,   2.87059918e-02\n",
      "   -1.81460436e-02   3.52541208e-02]\n",
      " [  5.04040681e-02   6.10041022e-02  -2.35801972e-02 ...,   2.73605976e-02\n",
      "   -1.85797513e-02   3.80267538e-02]]\n",
      "[ 0.35852671 -0.00240551  0.23412101 -0.27693102 -0.27561551  0.09518238\n",
      " -0.47256222 -0.3220686  -0.05376979 -0.15227795 -0.19409613  0.38227049\n",
      " -0.08657745  0.05554811  0.25913793  0.04887084 -0.18038048  0.13536933\n",
      "  0.11126575 -0.31087086 -0.35304505 -0.23729743  0.30959997 -0.10824584\n",
      " -0.22703582  0.3261404  -0.06244228 -0.02038438 -0.23482718 -0.2586126\n",
      "  0.22456212 -0.02549284  0.534325   -0.11899835  0.49499673 -0.00434911\n",
      "  0.05341128  0.09595773 -0.11196084  0.3603856   0.05189871 -0.09590574\n",
      " -0.43365029 -0.3392871   0.11852464  0.43623465 -0.24685729  0.13688494\n",
      " -0.07897767  0.11481569  0.18817562 -0.2083717  -0.06000104 -0.08697541\n",
      " -0.14797689 -0.19018193 -0.13867164  0.05580625  0.2265071  -0.17228045\n",
      "  0.11711054  0.32033944  0.20569232  0.21976322 -0.24246468 -0.16951674\n",
      "  0.28697932  0.24693701  0.04316518 -0.29618698 -0.4907223  -0.23433962\n",
      "  0.03467003 -0.2804521  -0.01194684 -0.12972689 -0.23996836  0.20683949\n",
      " -0.17436688 -0.51782703 -0.1366064   0.07008482  0.06763237  0.27500415\n",
      " -0.44281986  0.26722595 -0.23040846 -0.12243638  0.55106819  0.03732378\n",
      "  0.26089215 -0.2739048  -0.28733078 -0.17118898  0.25842285  0.10017424\n",
      " -0.09154076  0.13591349 -0.30656284 -0.17951186  0.21101078 -0.24036594\n",
      " -0.3405174   0.17078131 -0.03637201 -0.08638397 -0.10984948  0.2660926\n",
      " -0.18260783  0.06141059 -0.32494703  0.06207875  0.1356699  -0.13979468\n",
      " -0.05060187  0.31439197  0.38368243 -0.01658123  0.49213928  0.09101654\n",
      " -0.16646048  0.12153298 -0.12631316  0.05560859 -0.19637494 -0.17192304\n",
      "  0.14962159  0.41140783]\n",
      "[ 0.01595915]\n",
      "[[-0.02245172 -0.02548326  0.0022446  ...,  0.0006183  -0.01543238\n",
      "  -0.01666244]\n",
      " [-0.02245172 -0.02548326  0.0022446  ...,  0.0006183  -0.01543238\n",
      "  -0.01666244]\n",
      " [-0.02245172 -0.02548326  0.0022446  ...,  0.0006183  -0.01543238\n",
      "  -0.01666244]\n",
      " ..., \n",
      " [-0.03060058 -0.02183737  0.03640448 ...,  0.00505412  0.03350545\n",
      "  -0.04154873]\n",
      " [-0.02171965 -0.02568547 -0.0069219  ..., -0.03658875 -0.04849112\n",
      "  -0.04006836]\n",
      " [-0.02171965 -0.02568547 -0.0069219  ..., -0.03658875 -0.04849112\n",
      "  -0.04006836]]\n",
      "[[-0.00757131 -0.00470827 -0.00695666 ..., -0.00507791  0.00732832\n",
      "   0.00131344]\n",
      " [-0.01110628 -0.00750179 -0.00991493 ..., -0.00655024  0.01175272\n",
      "   0.00154063]\n",
      " [-0.01271816 -0.0091524  -0.0110205  ..., -0.00670517  0.01466252\n",
      "   0.00143259]\n",
      " ..., \n",
      " [ 0.00804632  0.00978373 -0.0068067  ..., -0.01126235 -0.01131291\n",
      "  -0.01028248]\n",
      " [ 0.00504601  0.01126604 -0.00981686 ..., -0.01916245 -0.01180373\n",
      "  -0.01035797]\n",
      " [ 0.00283477  0.01145854 -0.00949088 ..., -0.02152541 -0.01216378\n",
      "  -0.01071852]]\n",
      "[-0.29129034  0.23685952  0.35061896 -0.12272339  0.28519565 -0.16987483\n",
      " -0.07972074 -0.27460504 -0.16258529 -0.15045302 -0.03949349 -0.08677863\n",
      " -0.03224807 -0.00851675 -0.13262111  0.22793786  0.18082291  0.00338664\n",
      "  0.26007283  0.00244116 -0.29259259 -0.21008284  0.22599252 -0.23886138\n",
      " -0.21441871 -0.27062029 -0.26610768  0.01239299  0.00452141 -0.07673833\n",
      "  0.32884082  0.03205886  0.00277916  0.15259376 -0.13039383  0.00399517\n",
      "  0.02240981 -0.13454244 -0.16443984  0.2554037   0.25586724  0.10906593\n",
      " -0.32127899  0.035464    0.18638949 -0.0830728   0.35332114  0.34683263\n",
      " -0.09146163  0.19960594  0.19410136 -0.0337643  -0.02980465 -0.0673423\n",
      "  0.14732876  0.20183049  0.25085139 -0.17860085 -0.01390519 -0.14879659\n",
      " -0.30415034  0.22275466 -0.24981225  0.19381768 -0.08891883 -0.03718161\n",
      " -0.30523732 -0.04011923  0.13926761  0.23359174 -0.1010242  -0.22997104\n",
      " -0.02222986 -0.19809106 -0.25864506 -0.01001767 -0.01876224  0.09049928\n",
      "  0.11626215  0.00733072  0.0146267  -0.06521015  0.17064133  0.09619054\n",
      "  0.11937986  0.09432928 -0.15861796  0.11126314 -0.31425864 -0.16970463\n",
      "  0.120041    0.02523955 -0.02633335  0.26928347 -0.08138525  0.06221493\n",
      "  0.13079534 -0.15732549 -0.02320216  0.0441064  -0.06928864 -0.09639563\n",
      " -0.02261347 -0.16213463  0.26080388  0.16483387  0.26073208 -0.29144239\n",
      "  0.12180692  0.14653069 -0.17091835 -0.15372959  0.17573717 -0.14204429\n",
      " -0.23337899 -0.15975104  0.01966163  0.00901551 -0.04321977  0.31803417\n",
      " -0.19578126 -0.11977672  0.0792101  -0.27650347 -0.17406932  0.10926323\n",
      " -0.004       0.13054489]\n",
      "[ 0.14755]\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "n = 0\n",
    "\n",
    "for network in dump:\n",
    "    n += 1\n",
    "    l = 0\n",
    "    for layer in network:\n",
    "        l += 1\n",
    "        image = np.squeeze(layer)\n",
    "        plt.plot(image)\n",
    "        plt.savefig(\"model\"+str(n)+\"-\"+\"layer\"+str(l))\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez(\"dump\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
